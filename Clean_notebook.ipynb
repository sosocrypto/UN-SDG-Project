{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14173361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import re\n",
    "import pickle\n",
    "#import pycountry\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from langdetect import detect\n",
    "\n",
    "# #gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# libraries for NLP\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# importing libraries for utility from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# importing libraries for modelling form sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# libraries to check model metrics from sklearn\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report,accuracy_score\n",
    "\n",
    "# Enable interactive visualisations in Jupyter\n",
    "from ipywidgets import interact, interact_manual, widgets\n",
    "import plotly.express as px\n",
    "\n",
    "# Suppressing unnwarranted warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2afb991e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "     ------------------------------------ 126.0/126.0 kB 389.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (4.0.0)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3005d680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     -------------------------------------- 981.5/981.5 kB 1.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langdetect) (1.15.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=0bd619b67f21c3bf25385aee890df7a33920928715e47104747a472035ee88b2\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\13\\c7\\b0\\79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0ceaf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages for tweet scraping\n",
    "from datetime import date,timedelta,datetime\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import time\n",
    "# import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "# scheduler\n",
    "import schedule\n",
    "import os.path\n",
    "# aws\n",
    "# nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c74548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting schedule\n",
      "  Downloading schedule-1.1.0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: schedule\n",
      "Successfully installed schedule-1.1.0\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e66d44",
   "metadata": {},
   "source": [
    "### Tweet Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c767f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_tweets():\n",
    "    ''' Fetch all tweets from 2017 september based on the keywords provided, it returns a dataframe'''\n",
    "    tweets_list2 = []\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'(shell affordable energy) OR (shell clean energy) OR (shell affordable clean energy)  OR (shell decent work) OR (shell economic growth) OR (shell decent work economic growth) OR (shell industry innovation infrastructure) OR (shell innovation) OR (shell industry innovation) OR (shell responsible consumption) OR (shell responsible production) OR (shell climate action) OR (shell parternship) OR (shell partnership for goals)  lang:en since:2017-09-01 until:{date.today()}').get_items()):\n",
    "        print(f\"Tweeets {tweet.date}  {tweet.id}\")\n",
    "        tweets_list2.append([tweet.date, tweet.id, tweet.rawContent, tweet.user.username,tweet.user.verified,tweet.user.location,tweet.replyCount,tweet.retweetCount,tweet.likeCount,tweet.quoteCount])\n",
    "        tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet_Id', 'Text', 'Username','Verified','Location','Reply_Count','Retweet_Count','Like_Count','Quote_Count'])\n",
    "    return tweets_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sectioned_tweets(tweets_df):\n",
    "    '''Fetch tweets added since the time the last data was fetched and return a dataframe'''\n",
    "    # read the already existing csv file\n",
    "    df = tweets_df\n",
    "    # Check maximum date\n",
    "    d = datetime.strptime(df['Datetime'].agg('max')[0:-6], \"%Y-%m-%d %H:%M:%S\")\n",
    "    max_date = int(time.mktime(d.timetuple())+1)\n",
    "    now_time = int((datetime.now()+timedelta(days=1)).timestamp())\n",
    "    sectioned_tweet_list = []\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'(shell affordable energy) OR (shell clean energy) OR (shell affordable clean energy)  OR (shell decent work) OR (shell economic growth) OR (shell decent work economic growth) OR (shell industry innovation infrastructure) OR (shell innovation) OR (shell industry innovation) OR (shell responsible consumption) OR (shell responsible production) OR (shell climate action) OR (shell parternship) OR (shell partnership for goals) lang:en  since_time:{max_date} until_time:{now_time}').get_items()):\n",
    "        sectioned_tweet_list.append([tweet.date, tweet.id, tweet.rawContent, tweet.user.username,tweet.user.verified,tweet.user.location,tweet.replyCount,tweet.retweetCount,tweet.likeCount,tweet.quoteCount])\n",
    "        sectioned_tweets_df = pd.DataFrame(sectioned_tweet_list, columns=['Datetime', 'Tweet_Id', 'Text', 'Username','Verified','Location','Reply_Count','Retweet_Count','Like_Count','Quote_Count'])\n",
    "\n",
    "    return sectioned_tweets_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219247e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(tweet_df):\n",
    "    '''Clean data: i. drop duplicates ii. remove punctuations iii. Remove characters less than 3, it returns a dataframe '''\n",
    "    # remove duplicates\n",
    "    bul = tweet_df.duplicated(subset='Tweet_Id', keep='first' )\n",
    "    tweets_df2=tweet_df[~bul]\n",
    "    # Remove characters less than 3 and all mentions\n",
    "    tweets_df2['text_without_punctuations']= tweets_df2['Text'].apply(lambda x: \" \".join ([w for w in x.split() if len (w)>1 and '@' not in w[0:3] and 'amp' not in w]))\n",
    "     # remove punctuations  \n",
    "    pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "    subs_url = \" \"\n",
    "    tweets_df2['text_without_punctuations'] = tweets_df2['text_without_punctuations'].replace(to_replace = pattern_url, \n",
    "                                          value = subs_url, regex = True)\n",
    "    tweets_df2['text_without_punctuations']= tweets_df2['text_without_punctuations'].str.replace(\"[^'a-zA-Z0–9]\", ' ')\n",
    "    return tweets_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd62bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to be written \n",
    "# 1. fetch and create dataframe\n",
    "# 2. Compare already existing dataframe to see if there is any additional records\n",
    "# 3. drop duplicates\n",
    "# 4. remove punctuations\n",
    "# 5. tokenize\n",
    "# 6. remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e46af9",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4bf0e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Data read_csvtion, importing `csv` file only\n",
    "    Args:\n",
    "        path: string like `file` address in the directory\n",
    "    Return:\n",
    "        df: (Dataframe) output imported csv file as pandas Dataframe\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "    data = data[['Datetime', 'Text', 'Username','Verified', 'Location', 'Reply_Count','Retweet_Count', 'Like_Count', \n",
    "                 'Quote_Count']] # Select columns to work with\n",
    "    data = data.drop_duplicates( subset = 'Text') # drop duplicates tweets\n",
    "    #data = data[data['Text'].apply(detect).eq('en')] # filter non-english tweets\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb78682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = import_dataset('tweets_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cea2183d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Verified</th>\n",
       "      <th>Location</th>\n",
       "      <th>Reply_Count</th>\n",
       "      <th>Retweet_Count</th>\n",
       "      <th>Like_Count</th>\n",
       "      <th>Quote_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-20 12:28:48+00:00</td>\n",
       "      <td>or we could invest in renewables and not worry...</td>\n",
       "      <td>bertytrouser</td>\n",
       "      <td>False</td>\n",
       "      <td>Wales, United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-20 11:46:32+00:00</td>\n",
       "      <td>Sabine Brink, Blockchain CoE Lead at Shell giv...</td>\n",
       "      <td>Blockchain_Expo</td>\n",
       "      <td>False</td>\n",
       "      <td>Global</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Datetime  \\\n",
       "0  2022-09-20 12:28:48+00:00   \n",
       "1  2022-09-20 11:46:32+00:00   \n",
       "\n",
       "                                                Text         Username  \\\n",
       "0  or we could invest in renewables and not worry...     bertytrouser   \n",
       "1  Sabine Brink, Blockchain CoE Lead at Shell giv...  Blockchain_Expo   \n",
       "\n",
       "   Verified               Location  Reply_Count  Retweet_Count  Like_Count  \\\n",
       "0     False  Wales, United Kingdom            0              0           0   \n",
       "1     False                 Global            1              3          10   \n",
       "\n",
       "   Quote_Count  \n",
       "0            0  \n",
       "1            1  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a19e1f",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db2e1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "    '''\n",
    "    Function takes in the whole dataframe and carries out the following preprocessing steps:\n",
    "    \n",
    "    1. General text cleaning\n",
    "    2. Part of Speech tagging\n",
    "    3. Lemmatization\n",
    "    \n",
    "    Then return the dataframe\n",
    "    '''\n",
    "    def clean_tweet(tweet):\n",
    "        '''\n",
    "        tweet: String\n",
    "               Input Data\n",
    "        tweet: String\n",
    "               Output Data\n",
    "\n",
    "        func: Removes hashtag symbol in front of a word\n",
    "              Replace URLs with a space in the message\n",
    "              Replace ticker symbols with space. The ticker symbols are any stock symbol that starts with $.\n",
    "              Replace  usernames with space. The usernames are any word that starts with @.\n",
    "              Replace everything not a letter or apostrophe with space\n",
    "              Remove single letter words\n",
    "              filter all the non-alphabetic words, then join them again\n",
    "\n",
    "        '''\n",
    "    \n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "\n",
    "        tweet = re.sub('\\$[a-zA-Z0-9]*', ' ', tweet)\n",
    "        tweet = re.sub('https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*', ' ', tweet)\n",
    "        tweet = re.sub('\\@[a-zA-Z0-9]*', ' ', tweet)\n",
    "        tweet = re.sub('[^a-zA-Z\\']', ' ', tweet)\n",
    "        tweet = re.sub(r'\\s+', \" \", tweet)\n",
    "        tweet = ' '.join( [w for w in tweet.split() if len(w)>1] )\n",
    "    \n",
    "        return tweet\n",
    "    \n",
    "    def token_stop_pos(text):\n",
    "        '''\n",
    "        Maps the part of speech to words in sentences giving consideration to words that are nouns, verbs, \n",
    "        adjectives and adverbs\n",
    "        '''\n",
    "        pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "        tags = pos_tag(word_tokenize(text))\n",
    "        newlist = []\n",
    "        for word, tag in tags:\n",
    "            if word.lower() not in set(stopwords):\n",
    "                newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "        return newlist\n",
    "    \n",
    "    def lemmatize(pos_data):\n",
    "        '''\n",
    "        Performs lemmatization on tokens based on its part of speech tagging \n",
    "        '''\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        lemma_rew = \" \"\n",
    "        for word, pos in pos_data:\n",
    "            if not pos:\n",
    "                lemma = word\n",
    "                lemma_rew = lemma_rew + \" \" + lemma\n",
    "            else:\n",
    "                lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "                lemma_rew = lemma_rew + \" \" + lemma\n",
    "        return lemma_rew\n",
    "    \n",
    "    \n",
    "    df['clean_text'] = df['Text'].apply(lambda x:clean_tweet(x))\n",
    "    df['POS tagged'] = df['clean_text'].apply(token_stop_pos)\n",
    "    df['Lemma'] = df['POS tagged'].apply(lemmatize)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b63711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf8aa01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Verified</th>\n",
       "      <th>Location</th>\n",
       "      <th>Reply_Count</th>\n",
       "      <th>Retweet_Count</th>\n",
       "      <th>Like_Count</th>\n",
       "      <th>Quote_Count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>POS tagged</th>\n",
       "      <th>Lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-20 12:28:48+00:00</td>\n",
       "      <td>or we could invest in renewables and not worry...</td>\n",
       "      <td>bertytrouser</td>\n",
       "      <td>False</td>\n",
       "      <td>Wales, United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>or we could invest in renewables and not worry...</td>\n",
       "      <td>[(could, None), (invest, v), (renewables, n), ...</td>\n",
       "      <td>could invest renewables worry energy connect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-20 11:46:32+00:00</td>\n",
       "      <td>Sabine Brink, Blockchain CoE Lead at Shell giv...</td>\n",
       "      <td>Blockchain_Expo</td>\n",
       "      <td>False</td>\n",
       "      <td>Global</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Sabine Brink Blockchain CoE Lead at Shell givi...</td>\n",
       "      <td>[(Sabine, n), (Brink, n), (Blockchain, n), (Co...</td>\n",
       "      <td>Sabine Brink Blockchain CoE Lead Shell give ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-20 03:49:10+00:00</td>\n",
       "      <td>@NadineNakagawa @VancouverSun Shell is working...</td>\n",
       "      <td>moodyangela</td>\n",
       "      <td>False</td>\n",
       "      <td>Vancouver</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Shell is working with BCNDP and fed Libs to gr...</td>\n",
       "      <td>[(Shell, n), (working, v), (BCNDP, n), (fed, v...</td>\n",
       "      <td>Shell work BCNDP feed Libs greenwash natural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-19 20:54:22+00:00</td>\n",
       "      <td>Top 3 local firms Blue Shell Productions, Baja...</td>\n",
       "      <td>exportbarbados</td>\n",
       "      <td>False</td>\n",
       "      <td>Barbados</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Top local firms Blue Shell Productions Bajan D...</td>\n",
       "      <td>[(Top, a), (local, a), (firms, n), (Blue, n), ...</td>\n",
       "      <td>Top local firm Blue Shell Productions Bajan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-19 19:04:43+00:00</td>\n",
       "      <td>VIAVIWEB Wallpaper Admin SQL Injection / Shell...</td>\n",
       "      <td>_silvino_</td>\n",
       "      <td>False</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>VIAVIWEB Wallpaper Admin SQL Injection Shell U...</td>\n",
       "      <td>[(VIAVIWEB, n), (Wallpaper, n), (Admin, n), (S...</td>\n",
       "      <td>VIAVIWEB Wallpaper Admin SQL Injection Shell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16066</th>\n",
       "      <td>2017-09-02 20:55:25+00:00</td>\n",
       "      <td>@Java4Dayz @Shell @LiveWIREIntl Mark Twain by ...</td>\n",
       "      <td>Peer_Review1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Twain by definitions Clean Energy is fund...</td>\n",
       "      <td>[(Mark, n), (Twain, n), (definitions, n), (Cle...</td>\n",
       "      <td>Mark Twain definition Clean Energy fundament...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16067</th>\n",
       "      <td>2017-09-02 20:54:11+00:00</td>\n",
       "      <td>@Java4Dayz @Shell @LiveWIREIntl Mark Twain att...</td>\n",
       "      <td>Peer_Review1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Twain attached is proof showing energy is...</td>\n",
       "      <td>[(Mark, n), (Twain, n), (attached, v), (proof,...</td>\n",
       "      <td>Mark Twain attach proof show energy thermody...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16069</th>\n",
       "      <td>2017-09-02 00:34:09+00:00</td>\n",
       "      <td>#entrepeneurs Pak businesses short listed for ...</td>\n",
       "      <td>SocialMediaBoff</td>\n",
       "      <td>False</td>\n",
       "      <td>Barcelona, Catalunya</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>entrepeneurs Pak businesses short listed for S...</td>\n",
       "      <td>[(entrepeneurs, n), (Pak, n), (businesses, n),...</td>\n",
       "      <td>entrepeneurs Pak business short list Shell g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16070</th>\n",
       "      <td>2017-09-02 00:01:02+00:00</td>\n",
       "      <td>Pak businesses short listed for Shell global e...</td>\n",
       "      <td>EINShellNews</td>\n",
       "      <td>False</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Pak businesses short listed for Shell global e...</td>\n",
       "      <td>[(Pak, n), (businesses, n), (short, a), (liste...</td>\n",
       "      <td>Pak business short list Shell global entrepr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16072</th>\n",
       "      <td>2017-09-01 14:29:21+00:00</td>\n",
       "      <td>A sh/bash-style Unix shell built from scratch ...</td>\n",
       "      <td>silverbacklive</td>\n",
       "      <td>False</td>\n",
       "      <td>Dubai, United Arab Emirates</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sh bash style Unix shell built from scratch in...</td>\n",
       "      <td>[(sh, a), (bash, n), (style, n), (Unix, n), (s...</td>\n",
       "      <td>sh bash style Unix shell build scratch minut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15586 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Datetime  \\\n",
       "0      2022-09-20 12:28:48+00:00   \n",
       "1      2022-09-20 11:46:32+00:00   \n",
       "2      2022-09-20 03:49:10+00:00   \n",
       "3      2022-09-19 20:54:22+00:00   \n",
       "4      2022-09-19 19:04:43+00:00   \n",
       "...                          ...   \n",
       "16066  2017-09-02 20:55:25+00:00   \n",
       "16067  2017-09-02 20:54:11+00:00   \n",
       "16069  2017-09-02 00:34:09+00:00   \n",
       "16070  2017-09-02 00:01:02+00:00   \n",
       "16072  2017-09-01 14:29:21+00:00   \n",
       "\n",
       "                                                    Text         Username  \\\n",
       "0      or we could invest in renewables and not worry...     bertytrouser   \n",
       "1      Sabine Brink, Blockchain CoE Lead at Shell giv...  Blockchain_Expo   \n",
       "2      @NadineNakagawa @VancouverSun Shell is working...      moodyangela   \n",
       "3      Top 3 local firms Blue Shell Productions, Baja...   exportbarbados   \n",
       "4      VIAVIWEB Wallpaper Admin SQL Injection / Shell...        _silvino_   \n",
       "...                                                  ...              ...   \n",
       "16066  @Java4Dayz @Shell @LiveWIREIntl Mark Twain by ...     Peer_Review1   \n",
       "16067  @Java4Dayz @Shell @LiveWIREIntl Mark Twain att...     Peer_Review1   \n",
       "16069  #entrepeneurs Pak businesses short listed for ...  SocialMediaBoff   \n",
       "16070  Pak businesses short listed for Shell global e...     EINShellNews   \n",
       "16072  A sh/bash-style Unix shell built from scratch ...   silverbacklive   \n",
       "\n",
       "       Verified                     Location  Reply_Count  Retweet_Count  \\\n",
       "0         False        Wales, United Kingdom            0              0   \n",
       "1         False                       Global            1              3   \n",
       "2         False                    Vancouver            0              0   \n",
       "3         False                     Barbados            1              0   \n",
       "4         False                     Portugal            0              1   \n",
       "...         ...                          ...          ...            ...   \n",
       "16066     False                          NaN            0              0   \n",
       "16067     False                          NaN            0              0   \n",
       "16069     False         Barcelona, Catalunya            0              0   \n",
       "16070     False             Washington, D.C.            0              0   \n",
       "16072     False  Dubai, United Arab Emirates            0              0   \n",
       "\n",
       "       Like_Count  Quote_Count  \\\n",
       "0               0            0   \n",
       "1              10            1   \n",
       "2               0            0   \n",
       "3               0            0   \n",
       "4               0            0   \n",
       "...           ...          ...   \n",
       "16066           0            0   \n",
       "16067           0            0   \n",
       "16069           0            0   \n",
       "16070           0            0   \n",
       "16072           0            0   \n",
       "\n",
       "                                              clean_text  \\\n",
       "0      or we could invest in renewables and not worry...   \n",
       "1      Sabine Brink Blockchain CoE Lead at Shell givi...   \n",
       "2      Shell is working with BCNDP and fed Libs to gr...   \n",
       "3      Top local firms Blue Shell Productions Bajan D...   \n",
       "4      VIAVIWEB Wallpaper Admin SQL Injection Shell U...   \n",
       "...                                                  ...   \n",
       "16066  Mark Twain by definitions Clean Energy is fund...   \n",
       "16067  Mark Twain attached is proof showing energy is...   \n",
       "16069  entrepeneurs Pak businesses short listed for S...   \n",
       "16070  Pak businesses short listed for Shell global e...   \n",
       "16072  sh bash style Unix shell built from scratch in...   \n",
       "\n",
       "                                              POS tagged  \\\n",
       "0      [(could, None), (invest, v), (renewables, n), ...   \n",
       "1      [(Sabine, n), (Brink, n), (Blockchain, n), (Co...   \n",
       "2      [(Shell, n), (working, v), (BCNDP, n), (fed, v...   \n",
       "3      [(Top, a), (local, a), (firms, n), (Blue, n), ...   \n",
       "4      [(VIAVIWEB, n), (Wallpaper, n), (Admin, n), (S...   \n",
       "...                                                  ...   \n",
       "16066  [(Mark, n), (Twain, n), (definitions, n), (Cle...   \n",
       "16067  [(Mark, n), (Twain, n), (attached, v), (proof,...   \n",
       "16069  [(entrepeneurs, n), (Pak, n), (businesses, n),...   \n",
       "16070  [(Pak, n), (businesses, n), (short, a), (liste...   \n",
       "16072  [(sh, a), (bash, n), (style, n), (Unix, n), (s...   \n",
       "\n",
       "                                                   Lemma  \n",
       "0        could invest renewables worry energy connect...  \n",
       "1        Sabine Brink Blockchain CoE Lead Shell give ...  \n",
       "2        Shell work BCNDP feed Libs greenwash natural...  \n",
       "3        Top local firm Blue Shell Productions Bajan ...  \n",
       "4        VIAVIWEB Wallpaper Admin SQL Injection Shell...  \n",
       "...                                                  ...  \n",
       "16066    Mark Twain definition Clean Energy fundament...  \n",
       "16067    Mark Twain attach proof show energy thermody...  \n",
       "16069    entrepeneurs Pak business short list Shell g...  \n",
       "16070    Pak business short list Shell global entrepr...  \n",
       "16072    sh bash style Unix shell build scratch minut...  \n",
       "\n",
       "[15586 rows x 12 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5511e4",
   "metadata": {},
   "source": [
    "# Getting Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "68d42dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 55s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>or we could invest in renewables and not worry...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sabine Brink, Blockchain CoE Lead at Shell giv...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@NadineNakagawa @VancouverSun Shell is working...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text sentiment\n",
       "0  or we could invest in renewables and not worry...  Negative\n",
       "1  Sabine Brink, Blockchain CoE Lead at Shell giv...  Positive\n",
       "2  @NadineNakagawa @VancouverSun Shell is working...  Positive"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "def label_tweet(data):\n",
    "    '''function to predict sentiments using the Vader model\n",
    "    \n",
    "    Args: dataframe\n",
    "    '''\n",
    "    def vadersentimentanalysis(tweet):\n",
    "    \n",
    "        '''\n",
    "        calculates the sentiment score and returns it\n",
    "        '''\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "        vs = analyzer.polarity_scores(tweet)\n",
    "        compound = vs['compound']\n",
    "        return compound\n",
    "    \n",
    "    def vader_analysis(compound):\n",
    "    \n",
    "        '''\n",
    "        Maps the sentiment based on sentiment score as 'positive', 'negative' and 'neutral'\n",
    "        '''\n",
    "        if compound >= 0.5:\n",
    "            return 'Positive'\n",
    "        elif compound <= -0.5 :\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "    \n",
    "    data['sentiment_score'] = data['Lemma'].apply(vadersentimentanalysis)\n",
    "    data['sentiment'] = data['sentiment_score'].apply(vader_analysis)\n",
    "    data = data[['Text','sentiment']]\n",
    "    \n",
    "    return data\n",
    "labelled_data = label_tweet(data)\n",
    "labelled_data.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
